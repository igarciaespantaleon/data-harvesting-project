---
title: "Data Harvesting"
author: "Roc√≠o Galeote"
date: "2025-01-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loading libraries

```{r}
rm(list=ls()) 

library(scrapex)
library(rvest)
library(httr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(tidyverse)
library(RSelenium)
library(maps)
library(lubridate)
library(zoo)
library(fuzzyjoin)
library(stringdist)
library(purrr)
library(canadianmaps)
library(shiny)
library(leaflet)
library(plotly)

```

## First part: schools list

We are going to scrape the list of Residential Schools from the NCTR archive web. We need the school name, the years it was open, the location (Town, State) and the link to their individual page. 

```{r}

# Define the URL (test with first page)
url <- "https://nctr.ca/residential-schools/"

# Read the page
page <- read_html(url)

# Extract school names
school_names <- page %>%
  html_nodes("h2.text-xl a") %>%  # Targeting <h2> with class "text-xl", then <a>
  html_text(trim = TRUE)

# Extract school links
school_links <- page %>%
  html_nodes("h2.text-xl a") %>% 
  html_attr("href")

# Extract school details (location and years)
school_details <- page %>%
  html_nodes("p.text-sm.text-gray-500") %>%  # Select <p> with class "text-sm text-gray-500"
  html_text(trim = TRUE)

# Combine into a data frame
school_data <- tibble(
  school_name = school_names,
  school_link = school_links,
  details = school_details
)

# Print the results
print(school_data)


```

```{r}

# Base URLs
base_url <- "https://nctr.ca/residential-schools/"
paged_url <- "https://nctr.ca/residential-schools/page/"

# Function to scrape a single page
scrape_school_page <- function(page_num) {
  # Handle first page differently
  url <- ifelse(page_num == 1, base_url, paste0(paged_url, page_num, "/"))
  
  # Read the page
  page <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error fetching page: ", url)
    return(NULL)
  })
  
  if (is.null(page)) return(tibble(school_name = NA, school_link = NA, details = NA))
  
  # Extract school names
  school_names <- page %>%
    html_nodes("h2.text-xl a") %>%  
    html_text(trim = TRUE)
  
  # Extract school links
  school_links <- page %>%
    html_nodes("h2.text-xl a") %>% 
    html_attr("href")

  # Extract school details (location and years)
  school_details <- page %>%
    html_nodes("p.text-sm.text-gray-500") %>%  
    html_text(trim = TRUE)
  
  tibble(
    school_name = school_names,
    school_link = school_links,
    details = school_details
  )
}

# Scrape all pages (1 to 15)
all_schools <- map_dfr(1:15, scrape_school_page)

# View the final data
print(all_schools)

# Save to CSV
write.csv(all_schools, "residential_schools.csv", row.names = FALSE)

```


Now we have the school name, the link to each of them, the state and years of operation. But we also want to know the religious affiliation and the table of deceased students (if they have the info), which is contained within the individual link of each school.

```{r}

# Define a single school URL for testing
schooltest_url <- "https://nctr.ca/residential-schools/alberta/whitefish-lake-st-andrews/"

# Read the page
page <- tryCatch({
  read_html(schooltest_url)
}, error = function(e) {
  message("Error fetching page: ", schooltest_url)
  return(NULL)
})

if (!is.null(page)) {
  # Extract Religious Entity
  religious_entity <- page %>%
    html_nodes("p.max-w-2xl span.font-normal") %>% 
    html_text(trim = TRUE)
  
  # Extract Student Memorial Table
  table_node <- page %>% html_nodes("figure.wp-block-table table")

  if (length(table_node) > 0) {
    table_data <- table_node %>% 
      html_table(fill = TRUE) %>% 
      .[[1]]  # Extract first table if multiple exist
  } else {
    table_data <- tibble(Name = NA, Date_of_Death = NA)  # Placeholder if no table
  }

  # Print results
  print(glue::glue("Religious Entity: {religious_entity}"))
  print(table_data)
}

```


Now with all

```{r}
# Function to scrape a single school's details using its link

scrape_school_details <- function(school_link) {
  # Ensure the full URL is formed correctly
  full_url <- ifelse(grepl("^https?://", school_link), school_link, paste0("https://nctr.ca", school_link))
  
  # Read the page
  page <- tryCatch({
    read_html(full_url)
  }, error = function(e) {
    message("Error fetching page: ", full_url)
    return(NULL)
  })
  
  if (is.null(page)) return(tibble(school_link = school_link, religious_entity = NA, student_data = NA))
  
  # Extract Religious Entity
  religious_entity <- page %>%
    html_nodes("p.max-w-2xl span.font-normal") %>%
    html_text(trim = TRUE)
  
  # Extract Student Memorial Table
  table_node <- page %>% html_nodes("figure.wp-block-table table")
  
  if (length(table_node) > 0) {
    table_data <- table_node %>% 
      html_table(fill = TRUE) %>% 
      .[[1]]  # Extract first table if multiple exist
  } else {
    table_data <- tibble(Name = NA, Date_of_Death = NA)  # Placeholder if no table
  }
  
  tibble(
    school_link = school_link,
    religious_entity = ifelse(length(religious_entity) > 0, religious_entity, NA),
    student_data = list(table_data)  # Store tables in a list-column
  )
}

# Apply the scraping function to all school links from `all_schools`
detailed_school_data <- map_dfr(all_schools$school_link, scrape_school_details)

# View the results
print(detailed_school_data)

```

We join them by school_link

```{r}

# Join the two dataframes by the `school_link` column
combined_data <- left_join(all_schools, detailed_school_data, by = "school_link")

# View the combined data
print(combined_data)

```

We want to separate the details of each school into: school type, location and years_operation, and then remove the original column

```{r}


combined_data <- combined_data %>%
  mutate(
    # Extract School Type (before " - ")
    school_type = str_extract(details, "^[^ -]+"),

    # Extract Location (between " - " and the year in parentheses)
    location = str_extract(details, "(?<= - ).*(?= \\()"),

    # Extract Years of Operation (in parentheses)
    years_operation = str_extract(details, "\\(.*\\)")
  ) %>%
  # Remove the original 'details' column
  select(-details)

# View the updated combined data with the new columns
print(combined_data)
```


We need to clean the years_operation column, there is duplicate data. Also, there are weird spaces within the numbers and we need to standardize them. We chose to remove the parentheses too:

```{r}

combined_data <- combined_data %>%
  mutate(
    # Extract only the last set of parentheses with years
    years_operation = str_extract(years_operation, "\\([^()]*\\)$") %>%
      str_replace_all("[()]", "") %>%  # Remove parentheses
      str_trim() %>%  # Remove any leading or trailing spaces
      str_replace_all("\\s*[‚Äì‚Äî-]\\s*", "-")  # Standardize dashes to "-"
  )

# View the cleaned data
print(combined_data)
```


We're interested in seeing how many years each school was operating, so we extract the years from the column year_operation, we set them as numeric and we compute the substraction:

```{r}

combined_data <- combined_data %>%
  mutate(
    # Extract the first and last year as numeric values
    start_year = as.numeric(str_extract(years_operation, "^\\d{4}")),
    end_year = as.numeric(str_extract(years_operation, "\\d{4}$")),
    
    # Calculate years in operation (if both values exist)
    years_active = ifelse(!is.na(start_year) & !is.na(end_year),
                          end_year - start_year,
                          NA)
  ) %>%


# View the cleaned data
print(combined_data)

```



## Second part: Interactive map

Next, we go to the Arcgis map, because Selenium can't scrape an iframe that leads to another website. We detect how many markers there are in layer 9, we remove any spinner hidden in the code, locate the popup when clicking on a marker and returning the info on Title, Longitude and Latitude.


```{r}
library(RSelenium)

# Connect to Selenium running in Docker
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4449,
  browserName = "firefox"
)

remDr$open()

# Navigate to the ArcGIS map
remDr$navigate("https://www.arcgis.com/apps/webappviewer/index.html?id=0cc12b7f9f434dbaa7c2815aea84606e/")
Sys.sleep(5)  # Wait for page to load

# Find all markers in layer 9 (escape ID properly)
layer_9_markers <- remDr$findElements(using = "css selector", "#\\31 94bd5e080b-layer-9_layer > image")
print(paste("Layer 9 Markers found:", length(layer_9_markers)))


# üöÄ **Remove Hidden Spinner (Preemptive)**
remDr$executeScript("
  Array.from(document.querySelectorAll('.spinner.hidden')).forEach(el => el.remove());
")
Sys.sleep(5) 

for (marker in layer_9_markers) {
  Sys.sleep(2)  # Wait a bit before clicking
  marker$clickElement()
  
  # Ensure no popups are blocking
  close_buttons <- remDr$findElements(using = "css selector", "div.esriMobileNavigationBar img")
  if (length(close_buttons) > 0) {
    close_buttons[[1]]$clickElement()
    Sys.sleep(2)
  }

  # Remove spinner if it's back
  spinners <- remDr$findElements(using = "css selector", ".spinner.hidden")
  if (length(spinners) > 0) {
    remDr$executeScript("document.querySelectorAll('.spinner.hidden').forEach(el => el.remove());")
    Sys.sleep(1)
  }

  # Scroll to marker
  remDr$executeScript("arguments[0].scrollIntoView();", list(marker))
  Sys.sleep(1)

  # üöÄ **Remove Hidden Spinner Again**
  remDr$executeScript("
    Array.from(document.querySelectorAll('.spinner.hidden')).forEach(el => el.remove());
  ")

  # Locate the popup (single instance, since it updates content)
  popup <- remDr$findElement(using = "css selector", "div.esriPopupWrapper")

  remDr$screenshot(display = TRUE)  # Show image in RStudio Viewer

  # Extract the title from the header
  title_element <- popup$findChildElement(using = "css selector", ".header")
  title <- title_element$getElementText()[[1]]

  # Find all table rows in the popup
  rows <- popup$findChildElements(using = "css selector", ".attrTable tr")

  latitude <- "N/A"
  longitude <- "N/A"

  for (row in rows) {
    # Get columns (td elements) in each row
    cols <- row$findChildElements(using = "css selector", "td")

    if (length(cols) >= 2) {
      name_element <- cols[[1]]  # First column (attribute name)
      value_element <- cols[[2]] # Second column (attribute value)

      name_text <- name_element$getElementText()[[1]]
      value_text <- value_element$getElementText()[[1]]

      if (grepl("Latitude", name_text, ignore.case = TRUE)) {
        latitude <- value_text
      } else if (grepl("Longitude", name_text, ignore.case = TRUE)) {
        longitude <- value_text
      }
    }
  }

  print(paste("Title:", title, "Latitude:", latitude, "Longitude:", longitude))

# ‚úÖ **Close the Popup Before Moving to Next Marker**
tryCatch({
  close_button <- remDr$findElement(using = "css selector", "div.titleButton.close")
  close_button$clickElement()
  Sys.sleep(2)  # Allow time for the popup to close
}, error = function(e) {
  print("Popup close button not found or not clickable, moving on...")
})

}

  # Save data
school_data <- data.frame(Name = character(), Latitude = numeric(), Longitude = numeric(), stringsAsFactors = FALSE)

# Close the session
remDr$close()


```


This next part i dont know what its for, tbh

```{r}

# Connect to Selenium running in Docker
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4449,
  browserName = "firefox"
)

remDr$open()

# Navigate to the ArcGIS map
remDr$navigate("https://www.arcgis.com/apps/webappviewer/index.html?id=0cc12b7f9f434dbaa7c2815aea84606e/")
Sys.sleep(5)  # Wait for page to load

# Find all markers in layer 9 (escape ID properly)
layer_9_markers <- remDr$findElements(using = "css selector", "#\\31 94bd5e080b-layer-9_layer > image")
print(paste("Layer 9 Markers found:", length(layer_9_markers)))

# ‚úÖ Initialize dataframe BEFORE the loop
school_data <- data.frame(Name = character(), Latitude = numeric(), Longitude = numeric(), stringsAsFactors = FALSE)

# üöÄ **Remove Hidden Spinner (Preemptive)**
remDr$executeScript("
  Array.from(document.querySelectorAll('.spinner.hidden')).forEach(el => el.remove());
")
Sys.sleep(5) 

for (marker in layer_9_markers) {
  Sys.sleep(2)  # Wait a bit before clicking
  marker$clickElement()
  
  # Ensure no popups are blocking
  close_buttons <- remDr$findElements(using = "css selector", "div.esriMobileNavigationBar img")
  if (length(close_buttons) > 0) {
    close_buttons[[1]]$clickElement()
    Sys.sleep(2)
  }

  # Remove spinner if it's back
  spinners <- remDr$findElements(using = "css selector", ".spinner.hidden")
  if (length(spinners) > 0) {
    remDr$executeScript("document.querySelectorAll('.spinner.hidden').forEach(el => el.remove());")
    Sys.sleep(1)
  }

  # Scroll to marker
  remDr$executeScript("arguments[0].scrollIntoView();", list(marker))
  Sys.sleep(1)

  # üöÄ **Remove Hidden Spinner Again**
  remDr$executeScript("
    Array.from(document.querySelectorAll('.spinner.hidden')).forEach(el => el.remove());
  ")

  # Locate the popup (single instance, since it updates content)
  popup <- remDr$findElement(using = "css selector", "div.esriPopupWrapper")


  # Extract the title from the header
  title_element <- popup$findChildElement(using = "css selector", ".header")
  title <- title_element$getElementText()[[1]]

  # Find all table rows in the popup
  rows <- popup$findChildElements(using = "css selector", ".attrTable tr")

  latitude <- "N/A"
  longitude <- "N/A"

  for (row in rows) {
    # Get columns (td elements) in each row
    cols <- row$findChildElements(using = "css selector", "td")

    if (length(cols) >= 2) {
      name_element <- cols[[1]]  # First column (attribute name)
      value_element <- cols[[2]] # Second column (attribute value)

      name_text <- name_element$getElementText()[[1]]
      value_text <- value_element$getElementText()[[1]]

      if (grepl("Latitude", name_text, ignore.case = TRUE)) {
        latitude <- value_text
      } else if (grepl("Longitude", name_text, ignore.case = TRUE)) {
        longitude <- value_text
      }
    }
  }

  # ‚úÖ **Append the extracted data to the dataframe**
  school_data <- rbind(school_data, data.frame(Name = title, Latitude = latitude, Longitude = longitude, stringsAsFactors = FALSE))

  # ‚úÖ **Close the Popup Before Moving to Next Marker**
  tryCatch({
    close_button <- remDr$findElement(using = "css selector", "div.titleButton.close")
    close_button$clickElement()
    Sys.sleep(2)  # Allow time for the popup to close
  }, error = function(e) {
    print("Popup close button not found or not clickable, moving on...")
  })
}

# ‚úÖ Print final dataframe
print(school_data)

# ‚úÖ Save dataframe to CSV file
write.csv(school_data, "school_data.csv", row.names = FALSE)

# Close the session
remDr$close()

```


Works but we have 52 missing data when we shouldn't. We try a new approach scrolling the map. Printing each result in the console and screenshoting every time it scrolls.

```{r}

# Connect to Selenium running in Docker
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4449,
  browserName = "firefox"
)

remDr$open()

# Navigate to the ArcGIS map
remDr$navigate("https://www.arcgis.com/apps/webappviewer/index.html?id=0cc12b7f9f434dbaa7c2815aea84606e/")
Sys.sleep(5)  # Wait for page to load

# Remove hidden spinners
remDr$executeScript("
  Array.from(document.querySelectorAll('.spinner.hidden')).forEach(el => el.remove());
")
Sys.sleep(5) 

# Function to take a screenshot
take_screenshot <- function(filename) {
  screenshot_file <- paste0(filename, ".png")
  remDr$screenshot(file = screenshot_file)
  print(paste("üì∏ Screenshot saved:", screenshot_file))
}

# Function to move (pan) the map
move_map <- function(x_offset, y_offset, step_name) {
  print(paste("üó∫Ô∏è Moving map:", step_name))
  remDr$executeScript("
    let map = document.querySelector('.esriViewRoot'); 
    map.scrollBy(arguments[0], arguments[1]);", 
    list(x_offset, y_offset)
  )
  Sys.sleep(3)  # Allow time for map to load
  take_screenshot(paste0("map_moved_", step_name))
}

# Define scroll movements
movements <- list(
  list(x = 500, y = 0, name = "Right"),
  list(x = -1000, y = 0, name = "Left"),
  list(x = 500, y = 500, name = "Down"),
  list(x = 500, y = -1000, name = "Up")
)

# Function to scrape markers
scrape_markers <- function() {
  layer_9_markers <- remDr$findElements(using = "css selector", "#\\31 94bd5e080b-layer-9_layer > image")
  print(paste("üîç Total markers found:", length(layer_9_markers)))

  for (marker in layer_9_markers) {
    Sys.sleep(2)
    marker$clickElement()
    
    # Remove spinner if it reappears
    spinners <- remDr$findElements(using = "css selector", ".spinner.hidden")
    if (length(spinners) > 0) {
      remDr$executeScript("document.querySelectorAll('.spinner.hidden').forEach(el => el.remove());")
      Sys.sleep(1)
    }

    Sys.sleep(2)
    popups <- remDr$findElements(using = "css selector", "div.esriPopupWrapper")

    if (length(popups) == 0) {
      print("‚ö†Ô∏è Popup not found. Skipping marker.")
      next
    }

    popup <- popups[[1]]
    title_element <- popup$findChildElement(using = "css selector", ".header")
    title <- title_element$getElementText()[[1]]

    # Find all table rows in the popup
    rows <- popup$findChildElements(using = "css selector", ".attrTable tr")

    latitude <- "N/A"
    longitude <- "N/A"

    for (row in rows) {
      cols <- row$findChildElements(using = "css selector", "td")
      if (length(cols) >= 2) {
        name_text <- cols[[1]]$getElementText()[[1]]
        value_text <- cols[[2]]$getElementText()[[1]]

        if (grepl("Latitude", name_text, ignore.case = TRUE)) {
          latitude <- value_text
        } else if (grepl("Longitude", name_text, ignore.case = TRUE)) {
          longitude <- value_text
        }
      }
    }

    print(paste("‚úÖ Title:", title, "Latitude:", latitude, "Longitude:", longitude))

    # Close the popup
    tryCatch({
      close_button <- remDr$findElement(using = "css selector", "div.titleButton.close")
      close_button$clickElement()
      Sys.sleep(2)
    }, error = function(e) {
      print("Popup close button not found. Skipping closure.")
    })
  }
}

# üöÄ **Scrape first pass**
scrape_markers()

# üöÄ **Scroll the map in all directions**
for (move in movements) {
  move_map(move$x, move$y, move$name)
  scrape_markers()  # Try scraping again after each movement
}

# üöÄ **Final pass to ensure all markers were scraped**
scrape_markers()

# Save data
write.csv(scraped_markers, "scraped_markers.csv", row.names = FALSE)

# Close the session
remDr$close()


```


```{r}
# Connect to Selenium running in Docker
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4449,
  browserName = "firefox"
)

remDr$open()

# Navigate to the ArcGIS map
remDr$navigate("https://www.arcgis.com/apps/webappviewer/index.html?id=0cc12b7f9f434dbaa7c2815aea84606e/")
Sys.sleep(5)  # Wait for page to load


# Locate the Map Element
mapElement <- remDr$findElement(using = "xpath", value = "//*[@id='map']")  # Adjust the XPath as necessary

# Define Drag Offsets for each direction
drag_offsets <- list(
    list(startX = 0, startY = 0, endX = 500, endY = 0),   # Right
    list(startX = 500, startY = 0, endX = 500, endY = 500), # Down
    list(startX = 500, startY = 500, endX = 0, endY = 500), # Left
    list(startX = 0, startY = 500, endX = 0, endY = 0)      # Up
)

# Loop Through Offsets and Take Screenshots
for (i in seq_along(drag_offsets)) {
    offset <- drag_offsets[[i]]
    
    # Take Initial Screenshot
    remDr$screenshot(file = paste0("before_drag_", i, ".png"))
    print(paste("üì∏ Screenshot saved: before_drag_", i, ".png", sep = ""))

    # Define the JavaScript for dragging
    drag_script <- "
    var map = arguments[0];
    var mouseDownEvent = new MouseEvent('mousedown', {
        view: window,
        bubbles: true,
        cancelable: true,
        clientX: arguments[1],
        clientY: arguments[2]
    });
    map.dispatchEvent(mouseDownEvent);

    var mouseMoveEvent = new MouseEvent('mousemove', {
        view: window,
        bubbles: true,
        cancelable: true,
        clientX: arguments[3],
        clientY: arguments[4]
    });
    map.dispatchEvent(mouseMoveEvent);

    var mouseUpEvent = new MouseEvent('mouseup', {
        view: window,
        bubbles: true,
        cancelable: true,
        clientX: arguments[3],
        clientY: arguments[4]
    });
    map.dispatchEvent(mouseUpEvent);
    "

    # Execute the drag script with offsets
    remDr$executeScript(drag_script, list(mapElement, offset$startX, offset$startY, offset$endX, offset$endY))

    # Take Screenshot After Dragging
    remDr$screenshot(file = paste0("after_drag_", i, ".png"))
    print(paste("üì∏ Screenshot saved: after_drag_", i, ".png", sep = ""))
}


remDr$close()



```

## Data cleaning and processing

### Map data

The first step will be to import the csv file that resulted from the map's scraping. We also want to convert latitude and longitude to numeric format, and filter the rows that weren't scraped correctly.

```{r}
map_data <- read.csv("school_data.csv", sep = ",")

head(map_data)

map_data <- map_data |> 
  mutate(Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude)) 

map_data_clean <- map_data |> 
  filter(!is.na(Latitude) & !is.na(Longitude))

head(map_data_clean)
```

Since we scraped some entities that weren't actual Residential Schools, we wanted to check if the latitude and longitude of some of those entities had been successfully stored.

```{r}
map_data %>%
  filter(str_detect(Name, "Hearing|Confirmed"))
```
Unfortunately, we observe that all of those observations have missing latitude and longitude, so we won't use them.

The scraped table only has information on the coordinates, as well as the name, of the schools from the map, so we will have to resort to other tables to join any additional information we want to use. We quickly noticed that the school names that were scraped from the map didn't correspond to the ones scraped from the web archive in most cases, so we had perform some preprocessing to allow the join.

First, we clean map_data_clean by removing the prefix "Canadian Residential Schools: " from school names. Then, we filter out "Unknown Residential Schools" from combined_data since it does not correspond to a specific institution in the map data. 

```{r}
map_data_clean <- map_data_clean %>%
  mutate(Name = str_remove(Name, "^Canadian Residential Schools: "))


combined_data_join <- combined_data |> 
  select(school_name) |> 
  filter(school_name != "Unknown Residential Schools")
``` 

To handle disparities, we use a fuzzy join, which allows us to match schools even when their names have slight variations. We then refine the matches by selecting the closest corresponding name for each school (because, particularly due to the difference in length of the two data sets, some of the names were used for more than one match).

```{r}
# installed.packages("fuzzyjoin")
library(fuzzyjoin)

# Fuzzy join based on name similarity
correspondences <- stringdist_full_join(
  combined_data_join, 
  map_data_clean, 
  by = c("school_name" = "Name"),
  method = "jw",  # Jaro-Winkler similarity method
  max_dist = 0.25 
)

# keeping only the matches with minimum distance
library(stringdist)
correspondences <- correspondences %>%
  group_by(school_name) %>%
  filter(stringdist(school_name, Name, method = "jw") == min(stringdist(school_name, Name, method = "jw"))) %>%
  ungroup() |> 
  group_by(Name) %>%
  filter(stringdist(school_name, Name, method = "jw") == min(stringdist(school_name, Name, method = "jw"))) %>%
  ungroup()
```

Now, we can use a simple line of code to identify discrepancies.

```{r}
setdiff(map_data_clean$Name, correspondences$Name)
```

We manually checked that three of the names from the map that hadn't been identified by the fuzzyjoin, actually had a match in the web table, so we bind them.

```{r}
new_rows <- map_data_clean |> 
  filter(Name %in% c("Amos", "Kitimaat", "Wabasca (Athabasca Landing)")) |> 
  mutate(
  school_name = case_when(
    Name == "Amos" ~ "Amos (Saint-Marc-de-Figuery)", 
    Name == "Kitimaat" ~ "Kitimaat (Elizabeth Long Memorial Home for Girls)",
    Name == "Wabasca (Athabasca Landing)" ~ "Wabasca (St. John‚Äôs)"))

# Add the new rows to the correspondences dataframe and sort alphabetically
correspondences <- bind_rows(correspondences, new_rows) |> arrange(Name)

```

We merged the matched data back into map_data_clean, so that we have the two versions of school names and can perform future joins. With this step we lose the 7 schools that were scraped from the map but not found in the web (we searched for them manually without results, we don't know if they are known by other names or if they are missing from the archive altogether), ending up with 80 observations with data on coordinates.

### Web archive data

To work with the student data, we first need to extract the student data from the tibbles nested inside the detailed_school_data tibble that we created earlier, and perform some cleaning and preprocessing.

```{r}
expanded_data <- detailed_school_data %>%
  unnest(cols = student_data)

# two columns from the date of death were created, one of them with only NAs
colSums(!is.na(expanded_data))

expanded_data <- expanded_data %>%
  select(-`Date_of_Death`) %>% # remove the column with just NAs
  rename(date_of_death = `Date of Death (Year/Month/Day)`, # rename the one we're going to keep
full_name = Name) %>% # standardize column names
  left_join(combined_data %>%
              select(school_link, school_name), by = "school_link") %>% # the school_name column is missing from this table, so we use the links to join the names from combined_data
  select(school_name, everything()) # place the school name first

head(expanded_data)
```

We manually checked in the detailed_school_data set that some schools had empty tibbles for student data. We thought that for this data set it made sense to keep just the schools that have actual data about student deaths, even if more schools are present in the complete database.

```{r}
expanded_data %>%
  group_by(school_name) %>%
  summarise(
    total_rows = n(), # check if the schools with NAs have other cases or just empty tibbles
    na_full_name = sum(is.na(full_name)),
    na_date_of_death = sum(is.na(date_of_death))
  ) |> 
  filter(na_full_name > 0)
```

The are no explicit NAs in the tibble that DO NOT correspond to schools with empty tibbles in the original dataset (no cases documented). Removing NAs will remove all of those schools.

```{r}
expanded_data <- drop_na(expanded_data)
```

This dropped about 300 rows.

The next step is cleaning the date column:

```{r}
# first explore the different formats there are
expanded_data %>%
  select(date_of_death) |> 
  filter(!str_detect(date_of_death, "^\\d{4}-\\d{2}-\\d{2}$")) |> # filter out the dates on the standard YYYY-MM-DD format
  filter(date_of_death != "Not known") # there are some straight up missing
```

There are a few with a "ca. " format which we can get rid of because it provides no useful information.

This leaves us with five different possible formats:
- YYYY
- YYYY - YYYY
- YYYY-MM
- YYYY-MM-DD - YYYY-MM-DD (the interval can be months or even years long)
- YYYY-MM-DD or YYYY-MM-DD (the two dates don't need to be consecutive)

We decided to work with year-month format. Our goal was to be able to identify those deaths that had very long date intervals, and potentially exclude them if we deemed them less reliable for some parts of our analysis.

In the next chunk we added comments for all steps of this process.

```{r}
library(purrr)

expanded_data <- expanded_data %>%
  mutate(
    # Trim whitespace
    date_of_death = str_trim(date_of_death),
    # Standardize separators (long and short hyphens were present in the original data set)
    date_of_death = str_replace_all(date_of_death, "‚Äì", "-"),
    # Replace "or" by "-"
    date_of_death = str_replace_all(date_of_death, "\\s*or\\s*", " - "),
    # delete any "ca." expression
    date_of_death = str_replace_all(date_of_death, "ca\\.\\s*", ""),
    # Replace 31 by 30 in the day position to correct invalid dates (some rows had the 31st day for months that do not have 31 days, and this was preventing R from recognising them as dates)
    date_of_death = str_replace(date_of_death, "-31$", "-30"),
    # Convert full dates (YYYY-MM-DD) into a year-month format (YYYY-MM)
    date_of_death = str_replace_all(date_of_death, "(\\d{4}-\\d{2})-\\d{2}", "\\1")
  ) %>%
    # Handle date ranges by splitting values at " - "
  mutate(split_dates = str_split(date_of_death, "\\s+-\\s+")) %>%
  mutate(
    # Create date_start and date_end, ensuring that if only one date is provided, it is used as both start and end
    date_start = map_chr(split_dates, ~ .x[1]),
    date_end   = map_chr(split_dates, ~ if(length(.x) >= 2) .x[2] else .x[1])
  ) %>%
  select(-split_dates) %>%
  mutate(
    date_start = str_trim(date_start),
    date_end   = str_trim(date_end),
    # If only the year is provided, we assume January (-01) for date_start and December (-12) for date_end (for both YYYY and YYYY-YYYY formats)
    date_start = if_else(nchar(date_start) == 4, paste0(date_start, "-01"), date_start),
    date_end   = if_else(nchar(date_end)   == 4, paste0(date_end, "-12"), date_end)
  ) %>%
  # Convert into a year-month format
  mutate(across(c(date_start, date_end), ~ as.yearmon(.x, "%Y-%m")))
```

We run a test:
```{r}
expanded_data |> filter(is.na(date_end) & date_of_death != "Not known")
```

And find that there is only one case where the date_end column wasn't correctly filled, because there is a typo in the source, where the second date in date_of_death is 13. We can manually change it to 12 so that we don't lose this observation.

```{r}
expanded_data <- expanded_data %>%
  mutate(
    date_of_death = if_else(date_of_death == "1948-06 - 1949-13", "1948-06 - 1949-12", date_of_death),
    date_end = if_else(date_of_death == "1948-06 - 1949-12", as.yearmon("1949-12", "%Y-%m"), date_end)
  )
```

With these columns we can now check for death date intervals of any specific length:
```{r}
expanded_data |> filter(date_end - date_start > 0.5) # cases with a gap between possible death dates of more than 6 months
```

Though we still have many students whose dates of death are not known. 
```{r}
expanded_data %>%
  summarise(
    date_not_known = sum(date_of_death == "Not known"),
    date_start_na = sum(is.na(date_start)),
    date_end_na = sum(is.na(date_end))
  )
```
### Province data

Since we scraped location data for every school from the website, we can have information of the schools and deaths at the province level.

```{r}
combined_data <- combined_data %>%
  mutate(province = str_extract(location, "[A-Z]{2}$"))

combined_data %>%
  filter(is.na(province)) %>%
  select(school_name, location, province)
```

We have 3 NAs in the province column, the first one of which can be easily amended:

```{r}
combined_data$province[combined_data$school_name == "Red Deer Industrial School"] <- "AB"
```

The second one, Regina Industrial School, is mentioned to have been located in the province of Saskatchewan in the NCTR post, but it wasn't scraped correctly. We will manually add this information to have a more precise analysis.

```{r}
combined_data <- combined_data %>%
  mutate(
    location = case_when(
      school_name == "Regina" ~ "Regina, SK",
      TRUE ~ location  # Keep existing values
    ),
    province = case_when(
      school_name == "Regina" ~ "SK",
      TRUE ~ province  # Keep existing values
    )
  )
```

The last row logically has no location data, because it does not refer to a single residential school.

Next, we joined the province data to our existing expanded_data table:

```{r}
expanded_data <- expanded_data %>%
  left_join(combined_data %>% select(school_name, province), by = "school_name")
```

And calculated the deaths per each province.

```{r}
deaths_per_province <- expanded_data %>%
  filter(!is.na(province)) %>%
  group_by(province) %>%
  summarise(deaths = n())
```

We then merged this with the PROV object of the canadianmaps package so we could build a map for province data. 

```{r}
# Merge the deaths_per_province dataset with the province data
deaths_per_province <- merge(PROV, deaths_per_province, by.x = "PT", by.y = "province", all.x = TRUE)
```

### Tables for plots

In this section we will create some additional tables needed for the plots and maps of our shiny app.

First, a table that displays the total number of deaths, total number of schools, and deaths per school of each religious entity

```{r}
church_data <- expanded_data %>%
  group_by(religious_entity) %>%
  summarise(
    total_deaths = n(),
    total_schools = n_distinct(school_name)
  ) %>%
  mutate(deaths_per_school = total_deaths / total_schools) %>%
  arrange(desc(deaths_per_school))

setdiff(combined_data$religious_entity, church_data$religious_entity)

# Mennonite and Presbyterian Church aren't in the expanded_data table because they have no records of student deaths, however, we want to keep them with 0s in these columns

church_bind <- combined_data %>%
  filter(religious_entity %in% c("Mennonite", "Presbyterian Church")) %>%
  group_by(religious_entity) %>%
  summarise(
    total_schools = n_distinct(school_name),
    total_deaths = 0,  # No recorded deaths
    deaths_per_school = 0  # Avoid NaNs or divisions by zero
  )

church_data <- bind_rows(church_data, church_bind)
```

Here are some of the plots that we can create with this table:

```{r}

# ggplot(church_data, aes(x = reorder(religious_entity, total_deaths), y = total_deaths, fill = religious_entity)) +
#   geom_col(show.legend = FALSE) +
#   coord_flip() +  # Para facilitar la lectura
#   labs(
#     title = "Total number of deaths by religious entity",
#     x = "",
#     y = ""
#   ) +
#   theme_minimal()
# 
# 
# 
# ggplot(church_data, aes(x = reorder(religious_entity, deaths_per_school), y = deaths_per_school, fill = religious_entity)) +
#   geom_col(show.legend = FALSE) +
#   coord_flip() +
#   labs(
#     title = "Deaths per school by religious entity",
#     x = "",
#     y = ""
#   ) +
#   theme_minimal()
# 
# 
# 
# ggplot(church_data, aes(x = reorder(religious_entity, total_schools), y = total_schools, fill = religious_entity)) +
#   geom_col(show.legend = FALSE) +
#   coord_flip() +
#   labs(
#     title = "Number of schools by religious entity",
#     x = "",
#     y = ""
#   ) +
#   theme_minimal()

```

Using the table created in the previous section (deaths_per_province), we can now plot a map:

```{r}
# ggplot(deaths_per_province) +
#   geom_sf(aes(fill = deaths), colour = "white", size = 0.1) +
#   scale_fill_gradientn(
#     colours = c("lightblue", "blue", "darkblue"),  # Specify the gradient colors
#     values = c(0, 0.5, 1),  # Adjust these to match the range of your data
#     na.value = "gray",  # Gray for NA values
#     name = "", 
#     breaks = c(0, 200, 400, 600),  # Define breaks where you want labels
#     labels = c("No Data", "200", "400", "600")  # Provide the custom labels
#   ) +
#   theme_minimal() +
#   labs(title = "Number of deaths in Residential Schools by province") +
#   theme(
#     legend.position = "right",  # Position the legend at the bottom
#     legend.title = element_text(size = 10, face = "bold"),  # Bold the legend title
#     legend.text = element_text(size = 8),  # Adjust legend text size
#     plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),  # Center the title and make it bold
#     plot.margin = margin(10, 10, 10, 10),  # Add some margin around the plot
#     panel.grid = element_blank(),  # Remove gridlines
#     panel.border = element_blank(),  # Remove the border around the map
#     axis.text = element_blank(),  # Remove axis labels
#     axis.ticks = element_blank()  # Remove axis ticks
#   )

```
```{r}

# ggplot(combined_data %>% filter(!is.na(start_year) & !is.na(end_year)), 
#        aes(x = reorder(school_name, -start_year), y = start_year, color = religious_entity)) +
#   geom_segment(aes(xend = school_name, yend = end_year), size = 1) +  # Line from start to end year
#   geom_point(aes(y = end_year), size = 2) +  # Lollipop head
#   geom_point(aes(y = start_year), size = 2, shape = 1) +  # Open circle at start year
#   coord_flip() +  # Horizontal orientation
#   theme_minimal() +
#   labs(title = "Years of Operation of Schools",
#        x = "School Name",
#        y = "Year",
#        color = "Religious Entity") +
#   scale_color_viridis_d()

```

Now we can create a table that stores the yearly deaths of students for all schools. We extracted the years of the two columns we had created from the original date_of_death. If the years matched, we keep the first one (any of them would work, actually). If the years don't match, we convert it to NA.

```{r}
yearly_deaths <- expanded_data %>%
  mutate(
    year_start = year(date_start),
    year_end = year(date_end),
    year_of_death = if_else(year_start == year_end, year_start, NA_real_)
  ) %>%
  select(-year_start, -year_end)  # Remove temporary columns 

sum(is.na(yearly_deaths$year_of_death))
# we lose 371 of 2641 students

# Count the number of deaths per year
yearly_deaths <- yearly_deaths %>%
  drop_na() %>%
  group_by(year_of_death) %>%
  summarise(deaths = n())

ggplot(yearly_deaths, aes(x = year_of_death, y = deaths)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  scale_y_continuous(limits = c(0, 100)) +
  theme_minimal() +
  labs(title = "Yearly Deaths in Residential Schools",
       x = "Year",
       y = "Number of Deaths") +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
```

We join the data scraped from the map that we had processed earlier, with the rest of the information we have from the scraping of the web.

```{r}
deaths_data <- expanded_data %>%
  group_by(school_name) %>%
  summarise(deaths = n()) |>
  ungroup()

map_data_complete <- correspondences %>%
  left_join(
    combined_data |> select(school_name, religious_entity, school_link, location, years_operation),
    by = c("school_name" = "school_name")
  ) %>%
  left_join(
    deaths_data %>% select(school_name, deaths),
    by = c("school_name" = "school_name")
  ) |> 
  mutate(deaths = as.character(deaths))

map_data_complete[is.na(map_data_complete)] <- "Unknown"
```

```{r}
# tilemap_data <- expanded_data %>%
#   group_by(year_of_death, school_name) %>%
#   summarise(deaths = n()) |> 
#   ungroup()
# 
# ggplot(tilemap_data, aes(x = year_of_death, y = school_name, fill = deaths)) +
#   geom_tile(color = "white") +  # White borders for better separation
#   scale_fill_gradientn(
#     colours = c("darkorange", "brown", "darkred", "black"),
#     values = scales::rescale(c(0, 5, 10, 18)),  # Adjust for better contrast
#     na.value = "gray",
#     name = "Deaths"
#   ) +
#   theme_minimal() +
#   labs(title = "Heatmap of Deaths by School and Year",
#        x = "Year",
#        y = "School") +
#   theme(
#     axis.text.y = element_text(size = 8),  # Adjust text size
#     axis.text.x = element_text(angle = 45, hjust = 1),  # Tilt x-axis labels
#     legend.position = "right"
#   )
```

### Shiny app

```{r}
ui <- fluidPage(
  titlePanel("Residential Schools analysis"),
  
  tabsetPanel(
    # First tab: Bar plots
    tabPanel("Bar plots",
      sidebarLayout(
        sidebarPanel(
          # Dropdown to select which variable to display
          selectInput("variable", "Select variable to display:", 
                      choices = c("Number of schools" = "total_schools", 
                                  "Total number of deaths" = "total_deaths", 
                                  "Number of deaths per school" = "deaths_per_school"))
        ),
        mainPanel(
          plotOutput("plot", height = "700px")
        )
      )
    ),
    
    # Second tab: Interactive map
    tabPanel("Interactive map",
      leafletOutput("map", height = "700px")
    ),
    
    # Third tab: Static map (made sure the layout is fluid here)
    tabPanel("Static map",
      fluidRow(
        column(12, plotlyOutput("static_map", height = "700px"))  # Full width
      )
    ),
    
    # Fourth tab: Years of operation
    tabPanel("Years of operation", 
      fluidRow(
        column(12, plotlyOutput("lollipop_plot", height = "700px"))  # Full width
      )
    ),
    
    # Fifth tab: Yearly deaths
    tabPanel("Yearly deaths", 
      plotlyOutput("yearly_deaths_plot", height = "700px")  # Full width
    )
  )
)

server <- function(input, output, session) {
  
  # Render bar plot based on selected variable
  output$plot <- renderPlot({
    selected_var <- input$variable

    ggplot(relative_data, aes(x = religious_entity, y = get(selected_var))) +
      geom_bar(stat = "identity", fill = "skyblue") +
      labs(
        title = paste("Bar plot of", selected_var),
        x = "Religious Entity",
        y = selected_var
      ) +
      theme(
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        axis.text.x = element_text(size = 12, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 12)
      )
  })
  
  # Render Leaflet map
  output$map <- renderLeaflet({
    leaflet(data = map_data_complete) %>%
      addTiles() %>%
      addMarkers(
        lat = ~Latitude, 
        lng = ~Longitude, 
        popup = ~paste(
          "<b>School name:</b> ", school_name, "<br>",
          "<b>Religious entity:</b> ", religious_entity, "<br>",
          "<b>Location:</b> ", location, "<br>",
          "<b>Years of operation:</b> ", years_operation, "<br>",
          "<b>Number of deaths:</b> ", deaths
        ),
        popupOptions = popupOptions(maxWidth = 500, maxHeight = 300)
      )
  })
  
  # Render static map
  output$static_map <- renderPlotly({
    p <- ggplot(deaths_per_province) +
      geom_sf(aes(fill = deaths, text = paste("Province: ", PRENAME, "<br>Total Deaths: ", deaths)),
              colour = "white", size = 0.1) +
      scale_fill_gradientn(
        colours = c("lightblue", "blue", "darkblue"), 
        values = c(0, 0.5, 1),  
        na.value = "gray",  
        name = "", 
        breaks = c(0, 200, 400, 600),
        labels = c("No Data", "200", "400", "600")
      ) +
      theme_minimal() +
      labs(title = "Number of deaths in Residential Schools by province") +
      theme(
        legend.position = "right",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 8),
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        plot.margin = margin(10, 10, 10, 10),
        panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()
      )
    
    ggplotly(p, tooltip = "text")
  })

  output$lollipop_plot <- renderPlotly({
    custom_colors <- c(
      "Catholic" = "#f596e1",  
      "Anglican" = "#33a02c", 
      "Methodist" = "#ff6464", 
      "Methodist United Church" = "#cf1313",
      "Methodist United Church Catholic" = "#990e0e",
      "Presbyterian" = "#79bdf0",
      "Presbyterian Church" = "#2985ca", 
      "Presbyterian United Church" = "#185b8e",
      "United Church" = "#66307b",  
      "Baptist" = "#f1c232",
      "Mennonite" = "#b15928",
      "Non-Denominational" = "#999999" 
    )
  
    p <- ggplot(combined_data %>% filter(!is.na(start_year) & !is.na(end_year)), 
                aes(x = reorder(school_name, -start_year), y = start_year, 
                    color = religious_entity, 
                    text = paste0("School: ", school_name, 
                                  "<br>Years of operation: ", years_operation))) +  
      geom_segment(aes(xend = school_name, yend = end_year), size = 1) +  
      geom_point(aes(y = end_year), size = 2) +  
      geom_point(aes(y = start_year), size = 2, shape = 1) +  
      coord_flip() +  
      theme_minimal() +
      labs(title = "Years of operation of Residential Schools",
           x = "School name",
           y = "Year",
           color = "Religious entity") +
      scale_color_manual(values = custom_colors) +  # Apply custom colors
      theme(axis.text.y = element_blank())  # Hides school names on y-axis
    
    ggplotly(p, tooltip = "text")  
  })
  
  output$yearly_deaths_plot <- renderPlotly({
    plot_ly(deaths_per_year, x = ~year_of_death, y = ~deaths, type = 'scatter', mode = 'lines+markers', 
               text = ~paste("Year: ", year_of_death, "<br>Deaths: ", deaths),
          hoverinfo = 'text',
               line = list(color = 'steelblue', width = 2), 
               marker = list(color = 'steelblue', size = 6)) %>%
    layout(title = list(
      text = "Yearly deaths in Residential Schools",
      font = list(size = 14, family = 'Arial', face = 'bold')),
           xaxis = list(title = "Year"),
           yaxis = list(title = "Number of deaths", range = c(0, 100)),
           hovermode = "closest")
  })
}

shinyApp(ui, server)
```

