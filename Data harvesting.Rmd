---
title: "Data Harvesting"
author: "RocÃ­o Galeote"
date: "2025-01-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loading libraries

```{r}
rm(list=ls()) 

library(scrapex)
library(rvest)
library(httr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(tidyverse)
library(RSelenium)
library(maps)
library(lubridate)
library(zoo)
library(fuzzyjoin)
library(stringdist)
library(purrr)
library(canadianmaps)
library(shiny)
library(leaflet)
library(plotly)
library(DT)

```

## First part: schools list

We are going to scrape the list of Residential Schools from the NCTR archive web. We need the school name, the years it was open, the location (Town, State) and the link to their individual page. 

```{r}

# Define the URL (test with first page)
url <- "https://nctr.ca/residential-schools/"

# Read the page
page <- read_html(url)

# Extract school names
school_names <- page %>%
  html_nodes("h2.text-xl a") %>%  # Targeting <h2> with class "text-xl", then <a>
  html_text(trim = TRUE)

# Extract school links
school_links <- page %>%
  html_nodes("h2.text-xl a") %>% 
  html_attr("href")

# Extract school details (location and years)
school_details <- page %>%
  html_nodes("p.text-sm.text-gray-500") %>%  # Select <p> with class "text-sm text-gray-500"
  html_text(trim = TRUE)

# Combine into a data frame
school_data <- tibble(
  school_name = school_names,
  school_link = school_links,
  details = school_details
)

# Print the results
print(school_data)


```

```{r}

# Base URLs
base_url <- "https://nctr.ca/residential-schools/"
paged_url <- "https://nctr.ca/residential-schools/page/"

# Function to scrape a single page
scrape_school_page <- function(page_num) {
  # Handle first page differently
  url <- ifelse(page_num == 1, base_url, paste0(paged_url, page_num, "/"))
  
  # Read the page
  page <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error fetching page: ", url)
    return(NULL)
  })
  
  if (is.null(page)) return(tibble(school_name = NA, school_link = NA, details = NA))
  
  # Extract school names
  school_names <- page %>%
    html_nodes("h2.text-xl a") %>%  
    html_text(trim = TRUE)
  
  # Extract school links
  school_links <- page %>%
    html_nodes("h2.text-xl a") %>% 
    html_attr("href")

  # Extract school details (location and years)
  school_details <- page %>%
    html_nodes("p.text-sm.text-gray-500") %>%  
    html_text(trim = TRUE)
  
  tibble(
    school_name = school_names,
    school_link = school_links,
    details = school_details
  )
}

# Scrape all pages (1 to 15)
all_schools <- map_dfr(1:15, scrape_school_page)

# View the final data
print(all_schools)

# Save to CSV
write.csv(all_schools, "residential_schools.csv", row.names = FALSE)

```


Now we have the school name, the link to each of them, the state and years of operation. But we also want to know the religious affiliation and the table of deceased students (if they have the info), which is contained within the individual link of each school.

```{r}

# Define a single school URL for testing
schooltest_url <- "https://nctr.ca/residential-schools/alberta/whitefish-lake-st-andrews/"

# Read the page
page <- tryCatch({
  read_html(schooltest_url)
}, error = function(e) {
  message("Error fetching page: ", schooltest_url)
  return(NULL)
})

if (!is.null(page)) {
  # Extract Religious Entity
  religious_entity <- page %>%
    html_nodes("p.max-w-2xl span.font-normal") %>% 
    html_text(trim = TRUE)
  
  # Extract Student Memorial Table
  table_node <- page %>% html_nodes("figure.wp-block-table table")

  if (length(table_node) > 0) {
    table_data <- table_node %>% 
      html_table(fill = TRUE) %>% 
      .[[1]]  # Extract first table if multiple exist
  } else {
    table_data <- tibble(Name = NA, Date_of_Death = NA)  # Placeholder if no table
  }

  # Print results
  print(glue::glue("Religious Entity: {religious_entity}"))
  print(table_data)
}

```


Now with all

```{r}
# Function to scrape a single school's details using its link

scrape_school_details <- function(school_link) {
  # Ensure the full URL is formed correctly
  full_url <- ifelse(grepl("^https?://", school_link), school_link, paste0("https://nctr.ca", school_link))
  
  # Read the page
  page <- tryCatch({
    read_html(full_url)
  }, error = function(e) {
    message("Error fetching page: ", full_url)
    return(NULL)
  })
  
  if (is.null(page)) return(tibble(school_link = school_link, religious_entity = NA, student_data = NA))
  
  # Extract Religious Entity
  religious_entity <- page %>%
    html_nodes("p.max-w-2xl span.font-normal") %>%
    html_text(trim = TRUE)
  
  # Extract Student Memorial Table
  table_node <- page %>% html_nodes("figure.wp-block-table table")
  
  if (length(table_node) > 0) {
    table_data <- table_node %>% 
      html_table(fill = TRUE) %>% 
      .[[1]]  # Extract first table if multiple exist
  } else {
    table_data <- tibble(Name = NA, Date_of_Death = NA)  # Placeholder if no table
  }
  
  tibble(
    school_link = school_link,
    religious_entity = ifelse(length(religious_entity) > 0, religious_entity, NA),
    student_data = list(table_data)  # Store tables in a list-column
  )
}

# Apply the scraping function to all school links from `all_schools`
detailed_school_data <- map_dfr(all_schools$school_link, scrape_school_details)

# View the results
print(detailed_school_data)

```

We join them by school_link

```{r}

# Join the two dataframes by the `school_link` column
combined_data <- left_join(all_schools, detailed_school_data, by = "school_link")

# View the combined data
print(combined_data)

```

We want to separate the details of each school into: school type, location and years_operation, and then remove the original column

```{r}


combined_data <- combined_data %>%
  mutate(
    # Extract School Type (before " - ")
    school_type = str_extract(details, "^[^ -]+"),

    # Extract Location (between " - " and the year in parentheses)
    location = str_extract(details, "(?<= - ).*(?= \\()"),

    # Extract Years of Operation (in parentheses)
    years_operation = str_extract(details, "\\(.*\\)")
  ) %>%
  # Remove the original 'details' column
  select(-details)

# View the updated combined data with the new columns
print(combined_data)
```


We need to clean the years_operation column, there is duplicate data. Also, there are weird spaces within the numbers and we need to standardize them. We chose to remove the parentheses too:

```{r}

combined_data <- combined_data %>%
  mutate(
    # Extract only the last set of parentheses with years
    years_operation = str_extract(years_operation, "\\([^()]*\\)$") %>%
      str_replace_all("[()]", "") %>%  # Remove parentheses
      str_trim() %>%  # Remove any leading or trailing spaces
      str_replace_all("\\s*[â€“â€”-]\\s*", "-")  # Standardize dashes to "-"
  )

# View the cleaned data
print(combined_data)
```


We're interested in seeing how many years each school was operating, so we extract the years from the column year_operation, we set them as numeric and we compute the substraction:

```{r}

combined_data <- combined_data %>%
  mutate(
    # Extract the first and last year as numeric values
    start_year = as.numeric(str_extract(years_operation, "^\\d{4}")),
    end_year = as.numeric(str_extract(years_operation, "\\d{4}$")),
    
    # Calculate years in operation (if both values exist)
    years_active = ifelse(!is.na(start_year) & !is.na(end_year),
                          end_year - start_year,
                          NA)
  ) %>%


# View the cleaned data
print(combined_data)

```



## Second part: Interactive map

Next, we go to the Arcgis map, because Selenium can't scrape an iframe that leads to another website. We detect how many markers there are in layer 9, we remove any spinner hidden in the code, locate the popup when clicking on a marker and returning the info on Title, Longitude and Latitude.

Selenium navigates to the web, then clicks on the layers button, selects only the one we are interested in, then moves the map up 250 pixels and starts scraping.

```{r}
library(RSelenium)

# Connect to Selenium running in Docker
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4449,
  browserName = "firefox"
)

remDr$open()

# Navigate to the ArcGIS map
remDr$navigate("https://www.arcgis.com/apps/webappviewer/index.html?id=0cc12b7f9f434dbaa7c2815aea84606e/")
Sys.sleep(5)  # Wait for page to load

# Click on Layers button
layers_button <- remDr$findElement(using = "css selector", "#dijit__WidgetBase_4")
layers_button$clickElement()
Sys.sleep(3)


# Click on Drop Menu
drop_menu <- remDr$findElement(using = "css selector", "#jimu_dijit_DropMenu_0")
drop_menu$clickElement()
Sys.sleep(3)


# Click on the second option in the drop menu
second_option <- remDr$findElement(using = "css selector", "#jimu_dijit_DropMenu_0 > div.drop-menu > div:nth-child(2)")
second_option$clickElement()
Sys.sleep(3)


# Click on the checkbox
checkbox <- remDr$findElement(using = "css selector", "#jimu_dijit_CheckBox_0 > div.checkbox.jimu-float-leading.jimu-icon.jimu-icon-checkbox")
checkbox$clickElement()
Sys.sleep(3)


# Click on the close button
close_button <- remDr$findElement(using = "css selector", "#_32_panel > div.jimu-panel-title.jimu-main-background.title-normal > div > div.close-btn.jimu-vcenter")
close_button$clickElement()
Sys.sleep(3)


# Move the map **UP by 250 pixels**
print("Moving map Up 250 pixels")
map_element <- remDr$findElement(using = "css selector", "#map")
remDr$mouseMoveToLocation(webElement = map_element)
remDr$buttondown()
remDr$mouseMoveToLocation(x = 0, y = 250)
remDr$buttonup()
Sys.sleep(2)  # Allow time for the map to move

# ðŸ“¸ Screenshot after moving the map
remDr$screenshot(file = "step_7_map_moved.png")

# **SCRAPING MARKERS WITH ZOOM-IN HANDLING**
school_data <- data.frame(Name = character(), Latitude = numeric(), Longitude = numeric(), stringsAsFactors = FALSE)

layer_9_markers <- remDr$findElements(using = "css selector", "#\\31 94bd5e080b-layer-9_layer > image")
print(paste("Layer 9 Markers found:", length(layer_9_markers)))

for (marker in layer_9_markers) {
  Sys.sleep(2)
  marker$clickElement()
  Sys.sleep(5)

  # ðŸ“¸ Screenshot after clicking marker
  remDr$screenshot(display = TRUE)  # Show image in RStudio Viewer

  popup <- tryCatch({
    remDr$findElement(using = "css selector", "div.esriPopupWrapper")
  }, error = function(e) {
    return(NULL)
  })

  # If popup is missing, zoom in, retry, and then zoom out
  if (is.null(popup)) {
    print("Popup not found, zooming in...")
    
    zoom_in_button <- remDr$findElement(using = "css selector", ".esriSimpleSliderIncrementButton")
    zoom_in_button$clickElement()
    Sys.sleep(3)


    marker$clickElement()
    Sys.sleep(3)

    # ðŸ“¸ Screenshot after zooming in
    remDr$screenshot(display = TRUE)

    popup <- tryCatch({
      remDr$findElement(using = "css selector", "div.esriPopupWrapper")
    }, error = function(e) {
      return(NULL)
    })

    # Zoom back out if we zoomed in
    if (!is.null(popup)) {
      zoom_out_button <- remDr$findElement(using = "css selector", ".esriSimpleSliderDecrementButton")
      zoom_out_button$clickElement()
      Sys.sleep(3)

      # ðŸ“¸ Screenshot after zooming out
      remDr$screenshot(display = TRUE)
    }
  }

  if (!is.null(popup)) {
    title_element <- popup$findChildElement(using = "css selector", ".header")
    title <- title_element$getElementText()[[1]]

    rows <- popup$findChildElements(using = "css selector", ".attrTable tr")

    latitude <- "N/A"
    longitude <- "N/A"

    for (row in rows) {
      cols <- row$findChildElements(using = "css selector", "td")

      if (length(cols) >= 2) {
        name_text <- cols[[1]]$getElementText()[[1]]
        value_text <- cols[[2]]$getElementText()[[1]]

        if (grepl("Latitude", name_text, ignore.case = TRUE)) {
          latitude <- value_text
        } else if (grepl("Longitude", name_text, ignore.case = TRUE)) {
          longitude <- value_text
        }
      }
    }

    print(paste("Title:", title, "Latitude:", latitude, "Longitude:", longitude))

    # Store data
    school_data <- rbind(school_data, data.frame(Name = title, Latitude = as.numeric(latitude), Longitude = as.numeric(longitude), stringsAsFactors = FALSE))

    # Close popup before moving to the next marker
    tryCatch({
      close_button <- remDr$findElement(using = "css selector", "div.titleButton.close")
      close_button$clickElement()
      Sys.sleep(2)
    }, error = function(e) {
      print("Popup close button not found, moving on...")
    })
  }
}

# Save data
write.csv(school_data, "school_data.csv", row.names = FALSE)

# Close session
remDr$close()


```
Count how many NAs we have

```{r}

# Count NAs in each column
na_count_per_column <- colSums(is.na(school_data))
print("NA count per column:")
print(na_count_per_column)

# Count total NAs in the entire dataframe
total_na_count <- sum(is.na(school_data))
print(paste("Total NAs in the dataframe:", total_na_count))

```

## Data cleaning and processing

### Map data

The first step will be to import the csv file that resulted from the map's scraping. We also want to convert latitude and longitude to numeric format, and filter the rows that weren't scraped correctly.

```{r}
map_data <- read.csv("school_data.csv", sep = ",")

head(map_data)

map_data <- map_data |> 
  mutate(Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude)) 

map_data_clean <- map_data |> 
  filter(!is.na(Latitude) & !is.na(Longitude))

head(map_data_clean)
```

Since we scraped some entities that weren't actual Residential Schools, we wanted to check if the latitude and longitude of some of those entities had been successfully stored.

```{r}
map_data %>%
  filter(str_detect(Name, "Hearing|Confirmed"))
```
Unfortunately, we observe that all of those observations have missing latitude and longitude, so we won't use them.

The scraped table only has information on the coordinates, as well as the name, of the schools from the map, so we will have to resort to other tables to join any additional information we want to use. We quickly noticed that the school names that were scraped from the map didn't correspond to the ones scraped from the web archive in most cases, so we had perform some preprocessing to allow the join.

First, we clean map_data_clean by removing the prefix "Canadian Residential Schools: " from school names. Then, we filter out "Unknown Residential Schools" from combined_data since it does not correspond to a specific institution in the map data. 

```{r}
map_data_clean <- map_data_clean %>%
  mutate(Name = str_remove(Name, "^Canadian Residential Schools: "))


combined_data_join <- combined_data |> 
  select(school_name) |> 
  filter(school_name != "Unknown Residential Schools")
``` 

To handle disparities, we use a fuzzy join, which allows us to match schools even when their names have slight variations. We then refine the matches by selecting the closest corresponding name for each school (because, particularly due to the difference in length of the two data sets, some of the names were used for more than one match).

```{r}
# installed.packages("fuzzyjoin")
library(fuzzyjoin)

# Fuzzy join based on name similarity
correspondences <- stringdist_full_join(
  combined_data_join, 
  map_data_clean, 
  by = c("school_name" = "Name"),
  method = "jw",  # Jaro-Winkler similarity method
  max_dist = 0.25 
)

# keeping only the matches with minimum distance
library(stringdist)
correspondences <- correspondences %>%
  group_by(school_name) %>%
  filter(stringdist(school_name, Name, method = "jw") == min(stringdist(school_name, Name, method = "jw"))) %>%
  ungroup() |> 
  group_by(Name) %>%
  filter(stringdist(school_name, Name, method = "jw") == min(stringdist(school_name, Name, method = "jw"))) %>%
  ungroup()
```

Now, we can use a simple line of code to identify discrepancies.

```{r}
setdiff(map_data_clean$Name, correspondences$Name)
```

We manually checked that three of the names from the map that hadn't been identified by the fuzzyjoin, actually had a match in the web table, so we bind them.

```{r}
new_rows <- map_data_clean |> 
  filter(Name %in% c("Amos", "Kitimaat", "Wabasca (Athabasca Landing)")) |> 
  mutate(
  school_name = case_when(
    Name == "Amos" ~ "Amos (Saint-Marc-de-Figuery)", 
    Name == "Kitimaat" ~ "Kitimaat (Elizabeth Long Memorial Home for Girls)",
    Name == "Wabasca (Athabasca Landing)" ~ "Wabasca (St. Johnâ€™s)"))

# Add the new rows to the correspondences dataframe and sort alphabetically
correspondences <- bind_rows(correspondences, new_rows) |> arrange(Name)

```

We merged the matched data back into map_data_clean, so that we have the two versions of school names and can perform future joins. With this step we lose the 7 schools that were scraped from the map but not found in the web (we searched for them manually without results, we don't know if they are known by other names or if they are missing from the archive altogether), ending up with 80 observations with data on coordinates.

### Web archive data

To work with the student data, we first need to extract the student data from the tibbles nested inside the detailed_school_data tibble that we created earlier, and perform some cleaning and preprocessing.

```{r}
expanded_data <- detailed_school_data %>%
  unnest(cols = student_data)

# two columns from the date of death were created, one of them with only NAs
colSums(!is.na(expanded_data))

expanded_data <- expanded_data %>%
  select(-`Date_of_Death`) %>% # remove the column with just NAs
  rename(date_of_death = `Date of Death (Year/Month/Day)`, # rename the one we're going to keep
full_name = Name) %>% # standardize column names
  left_join(combined_data %>%
              select(school_link, school_name), by = "school_link") %>% # the school_name column is missing from this table, so we use the links to join the names from combined_data
  select(school_name, everything()) # place the school name first

head(expanded_data)
```

We manually checked in the detailed_school_data set that some schools had empty tibbles for student data. We thought that for this data set it made sense to keep just the schools that have actual data about student deaths, even if more schools are present in the complete database.

```{r}
expanded_data %>%
  group_by(school_name) %>%
  summarise(
    total_rows = n(), # check if the schools with NAs have other cases or just empty tibbles
    na_full_name = sum(is.na(full_name)),
    na_date_of_death = sum(is.na(date_of_death))
  ) |> 
  filter(na_full_name > 0)
```

The are no explicit NAs in the tibble that DO NOT correspond to schools with empty tibbles in the original dataset (no cases documented). Removing NAs will remove all of those schools.

```{r}
expanded_data <- drop_na(expanded_data)
```

This dropped about 300 rows.

The next step is cleaning the date column:

```{r}
# first explore the different formats there are
expanded_data %>%
  select(date_of_death) |> 
  filter(!str_detect(date_of_death, "^\\d{4}-\\d{2}-\\d{2}$")) |> # filter out the dates on the standard YYYY-MM-DD format
  filter(date_of_death != "Not known") # there are some straight up missing
```

There are a few with a "ca. " format which we can get rid of because it provides no useful information.

This leaves us with five different possible formats:
- YYYY
- YYYY - YYYY
- YYYY-MM
- YYYY-MM-DD - YYYY-MM-DD (the interval can be months or even years long)
- YYYY-MM-DD or YYYY-MM-DD (the two dates don't need to be consecutive)

We decided to work with year-month format. Our goal was to be able to identify those deaths that had very long date intervals, and potentially exclude them if we deemed them less reliable for some parts of our analysis.

In the next chunk we added comments for all steps of this process.

```{r}
# Save date of death in the original format to use later
original_death_date <- expanded_data |> 
  select(full_name, date_of_death) |> 
  rename(
    original_death_date = date_of_death
  )


# Transformations:

expanded_data <- expanded_data %>%
  mutate(
    # Trim whitespace
    date_of_death = str_trim(date_of_death),
    # Standardize separators (long and short hyphens were present in the original data set)
    date_of_death = str_replace_all(date_of_death, "â€“", "-"),
    # Replace "or" by "-"
    date_of_death = str_replace_all(date_of_death, "\\s*or\\s*", " - "),
    # delete any "ca." expression
    date_of_death = str_replace_all(date_of_death, "ca\\.\\s*", ""),
    # Replace 31 by 30 in the day position to correct invalid dates (some rows had the 31st day for months that do not have 31 days, and this was preventing R from recognising them as dates)
    date_of_death = str_replace(date_of_death, "-31$", "-30"),
    # Convert full dates (YYYY-MM-DD) into a year-month format (YYYY-MM)
    date_of_death = str_replace_all(date_of_death, "(\\d{4}-\\d{2})-\\d{2}", "\\1")
  ) %>%
    # Handle date ranges by splitting values at " - "
  mutate(split_dates = str_split(date_of_death, "\\s+-\\s+")) %>%
  mutate(
    # Create date_start and date_end, ensuring that if only one date is provided, it is used as both start and end
    date_start = map_chr(split_dates, ~ .x[1]),
    date_end   = map_chr(split_dates, ~ if(length(.x) >= 2) .x[2] else .x[1])
  ) %>%
  select(-split_dates) %>%
  mutate(
    date_start = str_trim(date_start),
    date_end   = str_trim(date_end),
    # If only the year is provided, we assume January (-01) for date_start and December (-12) for date_end (for both YYYY and YYYY-YYYY formats)
    date_start = if_else(nchar(date_start) == 4, paste0(date_start, "-01"), date_start),
    date_end   = if_else(nchar(date_end)   == 4, paste0(date_end, "-12"), date_end)
  ) %>%
  # Convert into a year-month format
  mutate(across(c(date_start, date_end), ~ as.yearmon(.x, "%Y-%m")))
```

We run a test:
```{r}
expanded_data |> filter(is.na(date_end) & date_of_death != "Not known")
```

And find that there is only one case where the date_end column wasn't correctly filled, because there is a typo in the source, where the second date in date_of_death is 13. We can manually change it to 12 so that we don't lose this observation.

```{r}
expanded_data <- expanded_data %>%
  mutate(
    date_of_death = if_else(date_of_death == "1948-06 - 1949-13", "1948-06 - 1949-12", date_of_death),
    date_end = if_else(date_of_death == "1948-06 - 1949-12", as.yearmon("1949-12", "%Y-%m"), date_end)
  )
```

With these columns we can now check for death date intervals of any specific length:
```{r}
expanded_data |> filter(date_end - date_start > 0.5) # cases with a gap between possible death dates of more than 6 months
```

Though we still have many students whose dates of death are not known. 
```{r}
expanded_data %>%
  summarise(
    date_not_known = sum(date_of_death == "Not known"),
    date_start_na = sum(is.na(date_start)),
    date_end_na = sum(is.na(date_end))
  )
```
### Province data

Since we scraped location data for every school from the website, we can have information of the schools and deaths at the province level.

```{r}
combined_data <- combined_data %>%
  mutate(province = str_extract(location, "[A-Z]{2}$"))

combined_data %>%
  filter(is.na(province)) %>%
  select(school_name, location, province)
```

We have 3 NAs in the province column, the first one of which can be easily amended:

```{r}
combined_data$province[combined_data$school_name == "Red Deer Industrial School"] <- "AB"
```

The second one, Regina Industrial School, is mentioned to have been located in the province of Saskatchewan in the NCTR post, but it wasn't scraped correctly. We will manually add this information to have a more precise analysis.

```{r}
combined_data <- combined_data %>%
  mutate(
    location = case_when(
      school_name == "Regina" ~ "Regina, SK",
      TRUE ~ location  # Keep existing values
    ),
    province = case_when(
      school_name == "Regina" ~ "SK",
      TRUE ~ province  # Keep existing values
    )
  )
```

The last row logically has no location data, because it does not refer to a single residential school.

Next, we joined the province data to our existing expanded_data table:

```{r}
expanded_data <- expanded_data %>%
  left_join(combined_data %>% select(school_name, province), by = "school_name")
```

And calculated the deaths per each province.

```{r}
deaths_per_province <- expanded_data %>%
  filter(!is.na(province)) %>%
  group_by(province) %>%
  summarise(
    total_deaths = n(),
    total_schools = n_distinct(school_name)
  ) %>%
  mutate(deaths_per_school = total_deaths / total_schools)
```

We then merged this with the PROV object of the canadianmaps package so we could build a map for province data. 

```{r}
# Merge the deaths_per_province dataset with the province data
deaths_per_province <- merge(PROV, deaths_per_province, by.x = "PT", by.y = "province", all.x = TRUE)
```

### Tables for plots

In this section we will create some additional tables needed for the plots and maps of our shiny app.

First, a table that displays the total number of deaths, total number of schools, and deaths per school of each religious entity

```{r}
church_data <- expanded_data %>%
  group_by(religious_entity) %>%
  summarise(
    total_deaths = n(),
    total_schools = n_distinct(school_name)
  ) %>%
  mutate(deaths_per_school = total_deaths / total_schools) %>%
  arrange(desc(deaths_per_school))

setdiff(combined_data$religious_entity, church_data$religious_entity)

# Mennonite and Presbyterian Church aren't in the expanded_data table because they have no records of student deaths, however, we want to keep them with 0s in these columns

church_bind <- combined_data %>%
  filter(religious_entity %in% c("Mennonite", "Presbyterian Church")) %>%
  group_by(religious_entity) %>%
  summarise(
    total_schools = n_distinct(school_name),
    total_deaths = 0,  # No recorded deaths
    deaths_per_school = 0  # Avoid NaNs or divisions by zero
  )

church_data <- bind_rows(church_data, church_bind)
```

Now we can create a table that stores the yearly deaths of students for all schools. We extracted the years of the two columns we had created from the original date_of_death. If the years matched, we keep the first one (any of them would work, actually). If the years don't match, we convert it to NA.

```{r}
yearly_deaths <- expanded_data %>%
  mutate(
    year_start = year(date_start),
    year_end = year(date_end),
    year_of_death = if_else(year_start == year_end, year_start, NA_real_)
  ) %>%
  select(-year_start, -year_end)  # Remove temporary columns 

sum(is.na(yearly_deaths$year_of_death))
# we lose 371 of 2641 students

# Count the number of deaths per year
yearly_deaths <- yearly_deaths %>%
  drop_na() %>%
  group_by(year_of_death) %>%
  summarise(deaths = n())
```

We join the data scraped from the map that we had processed earlier, with the rest of the information we have from the scraping of the web.

```{r}
deaths_data <- expanded_data %>%
  group_by(school_name) %>%
  summarise(deaths = n()) |>
  ungroup()

map_data_complete <- correspondences %>%
  left_join(
    combined_data |> select(school_name, religious_entity, school_link, location, years_operation),
    by = c("school_name" = "school_name")
  ) %>%
  left_join(
    deaths_data %>% select(school_name, deaths),
    by = c("school_name" = "school_name")
  ) |> 
  mutate(deaths = as.character(deaths))

map_data_complete[is.na(map_data_complete)] <- "Unknown"
```

Lastly, we created the data table that we want to display on our shiny app.

```{r}
shiny_data_table <- expanded_data |> 
  select(-c(date_of_death, date_start, date_end, province)) |> 
  left_join(original_death_date, by = "full_name") %>%
  left_join((combined_data %>%
              select(school_link, location)), by = "school_link") |> 
  rename(
    "School name" = school_name,
    "Religious entity" = religious_entity,
    "Full name" = full_name,
    "Date of death" = original_death_date,
    "Location" = location,
  )
```

### Shiny app

```{r}
ui <- fluidPage(
  titlePanel("Residential Schools analysis"),
  
  tabsetPanel(
    
    # First tab: Bar plots
    tabPanel("Bar plots",
      sidebarLayout(
        sidebarPanel(
          # Dropdown to select which variable to display
          selectInput("variable", "Select data to display:", 
                      choices = c("Number of schools" = "total_schools", 
                                  "Total number of deaths" = "total_deaths", 
                                  "Number of deaths per school" = "deaths_per_school"))
        ),
        mainPanel(
          plotOutput("plot", height = "700px")
        )
      )
    ),
    
        # Second tab: Static maps 
    tabPanel("Static map",
      fluidRow(
        column(4, 
          radioButtons("map_variable", "Select data to display:", 
            choices = c("Number of schools" = "total_schools", 
                       "Total number of deaths" = "total_deaths", 
                         "Number of deaths per school" = "deaths_per_school"),
                       selected = "total_schools")
        ),
        column(8, plotlyOutput("static_map", height = "700px"))
      )
    ),
    
    # Third tab: Interactive map
    tabPanel("Interactive map",
      leafletOutput("map", height = "700px")
    ),
    

    
    # Fourth tab: Years of operation
    tabPanel("Years of operation", 
      fluidRow(
        column(12, plotlyOutput("lollipop_plot", height = "700px"))  # Full width
      )
    ),
    
    # Fifth tab: Yearly deaths
    tabPanel("Yearly deaths", 
      plotlyOutput("yearly_deaths_plot", height = "700px")  # Full width
    ),
    
    # Sixth tab: Data table
    tabPanel("Data table", 
      fluidRow(
        column(12, DTOutput("data_table"))
  )
)
))

server <- function(input, output, session) {
  
  # Render bar plot based on selected variable
  output$plot <- renderPlot({
    selected_var <- input$variable

    ggplot(church_data, aes(x = religious_entity, y = get(selected_var))) +
      geom_bar(stat = "identity", fill = "skyblue") +
      geom_text(aes(label = round(get(selected_var), 1)), vjust = -0.5, size = 5) + 
      labs(
        title = paste("Bar plot of", selected_var),
        x = "Religious Entity",
        y = selected_var
      ) +
      theme(
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        axis.text.x = element_text(size = 12, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 12)
      )
  })

  
  # Render static map
  output$static_map <- renderPlotly({
  p <- ggplot(deaths_per_province) +
    geom_sf(aes(fill = .data[[input$map_variable]], 
                text = paste("Province: ", .data[["PRENAME"]], "<br>",
                             "Total deaths: ", .data[["total_deaths"]], "<br>",
                             "Total schools: ", .data[["total_schools"]], "<br>",
                             "Deaths per school: ", round(.data[["deaths_per_school"]], 2))),
            colour = "white", size = 0.1) +
    scale_fill_gradientn(
      colours = c("lightblue", "blue", "darkblue"), 
      values = c(0, 0.5, 1),  
      na.value = "gray",  
      name = input$map_variable, 
      breaks = c(0, 200, 400, 600),
      labels = c("No Data", "200", "400", "600")
    ) +
    theme_minimal() +
    labs(title = "Analysis of Residential Schools by province") +
    theme(
      legend.position = "none",
      legend.title = element_text(size = 10, face = "bold"),
      legend.text = element_text(size = 8),
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.margin = margin(10, 10, 10, 10),
      panel.grid = element_blank(),
      panel.border = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank()
    )

  ggplotly(p, tooltip = "text")
})
  
  # Render Leaflet map
  output$map <- renderLeaflet({
    leaflet(data = map_data_complete) %>%
      addTiles() %>%
      addMarkers(
        lat = ~Latitude, 
        lng = ~Longitude, 
        popup = ~paste(
          "<b>School name:</b> ", school_name, "<br>",
          "<b>Religious entity:</b> ", religious_entity, "<br>",
          "<b>Location:</b> ", location, "<br>",
          "<b>Years of operation:</b> ", years_operation, "<br>",
          "<b>Number of deaths:</b> ", deaths
        ),
        popupOptions = popupOptions(maxWidth = 500, maxHeight = 300)
      )
  })
  
  # Render lollipop chart
  output$lollipop_plot <- renderPlotly({
    custom_colors <- c(
      "Catholic" = "#f596e1",  
      "Anglican" = "#33a02c", 
      "Methodist" = "#ff6464", 
      "Methodist United Church" = "#cf1313",
      "Methodist United Church Catholic" = "#990e0e",
      "Presbyterian" = "#79bdf0",
      "Presbyterian Church" = "#2985ca", 
      "Presbyterian United Church" = "#185b8e",
      "United Church" = "#66307b",  
      "Baptist" = "#f1c232",
      "Mennonite" = "#b15928",
      "Non-Denominational" = "#999999" 
    )
  
    p <- ggplot(combined_data %>% filter(!is.na(start_year) & !is.na(end_year)), 
                aes(x = reorder(school_name, -start_year), y = start_year, 
                    color = religious_entity, 
                    text = paste0("School: ", school_name, 
                                  "<br>Years of operation: ", years_operation))) +  
      geom_segment(aes(xend = school_name, yend = end_year), size = 1) +  
      geom_point(aes(y = end_year), size = 2) +  
      geom_point(aes(y = start_year), size = 2, shape = 1) +  
      coord_flip() +  
      theme_minimal() +
      labs(title = "Years of operation of Residential Schools",
           x = "School name",
           y = "Year",
           color = "Religious entity") +
      scale_color_manual(values = custom_colors) +  # Apply custom colors
      theme(axis.text.y = element_blank())  # Hides school names on y-axis
    
    ggplotly(p, tooltip = "text")  
  })
  
  output$yearly_deaths_plot <- renderPlotly({
    plot_ly(yearly_deaths, x = ~year_of_death, y = ~deaths, type = 'scatter', mode = 'lines+markers', 
               text = ~paste("Year: ", year_of_death, "<br>Deaths: ", deaths),
          hoverinfo = 'text',
               line = list(color = 'steelblue', width = 2), 
               marker = list(color = 'steelblue', size = 6)) %>%
    layout(title = list(
      text = "Yearly deaths in Residential Schools",
      font = list(size = 14, family = 'Arial', face = 'bold')),
           xaxis = list(title = "Year"),
           yaxis = list(title = "Number of deaths", range = c(0, 100)),
           hovermode = "closest")
  })
  
output$data_table <- renderDT({
  datatable(shiny_data_table %>% 
              mutate(`School name` = paste0("<a href='", school_link, 
                                            "' target='_blank'>", `School name`, "</a>")) %>%
              select(-school_link),  # Remove school_link column
            escape = FALSE,   # Allow HTML links
            options = list(pageLength = 10), 
            rownames = FALSE)
})
}

shinyApp(ui, server)
```


