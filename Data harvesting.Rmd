---
title: "Data Harvesting"
author: "RocÃ­o Galeote"
date: "2025-01-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## First part: schools list

We are going to scrape the list of Residential Schools from the NCTR archive web. We need the school name, the years it was open, the location (Town, State) and the link to their individual page. 

```{r}
rm(list=ls()) 

library(scrapex)
library(rvest)
library(httr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(tidyverse)

```


```{r}

# Define the URL (test with first page)
url <- "https://nctr.ca/residential-schools/"

# Read the page
page <- read_html(url)

# Extract school names
school_names <- page %>%
  html_nodes("h2.text-xl a") %>%  # Targeting <h2> with class "text-xl", then <a>
  html_text(trim = TRUE)

# Extract school links
school_links <- page %>%
  html_nodes("h2.text-xl a") %>% 
  html_attr("href")

# Extract school details (location and years)
school_details <- page %>%
  html_nodes("p.text-sm.text-gray-500") %>%  # Select <p> with class "text-sm text-gray-500"
  html_text(trim = TRUE)

# Combine into a data frame
school_data <- tibble(
  school_name = school_names,
  school_link = school_links,
  details = school_details
)

# Print the results
print(school_data)


```

```{r}

# Base URLs
base_url <- "https://nctr.ca/residential-schools/"
paged_url <- "https://nctr.ca/residential-schools/page/"

# Function to scrape a single page
scrape_school_page <- function(page_num) {
  # Handle first page differently
  url <- ifelse(page_num == 1, base_url, paste0(paged_url, page_num, "/"))
  
  # Read the page
  page <- tryCatch({
    read_html(url)
  }, error = function(e) {
    message("Error fetching page: ", url)
    return(NULL)
  })
  
  if (is.null(page)) return(tibble(school_name = NA, school_link = NA, details = NA))
  
  # Extract school names
  school_names <- page %>%
    html_nodes("h2.text-xl a") %>%  
    html_text(trim = TRUE)
  
  # Extract school links
  school_links <- page %>%
    html_nodes("h2.text-xl a") %>% 
    html_attr("href")

  # Extract school details (location and years)
  school_details <- page %>%
    html_nodes("p.text-sm.text-gray-500") %>%  
    html_text(trim = TRUE)
  
  tibble(
    school_name = school_names,
    school_link = school_links,
    details = school_details
  )
}

# Scrape all pages (1 to 15)
all_schools <- map_dfr(1:15, scrape_school_page)

# View the final data
print(all_schools)

# Save to CSV
write.csv(all_schools, "residential_schools.csv", row.names = FALSE)

```


Now we have the school name, the link to each of them, the state and years of operation. But we also want to know the religious affiliation and the table of deceased students (if they have the info), which is contained within the individual link of each school.

```{r}

library(rvest)
library(tidyverse)
library(httr)

# Define a single school URL for testing
schooltest_url <- "https://nctr.ca/residential-schools/alberta/whitefish-lake-st-andrews/"

# Read the page
page <- tryCatch({
  read_html(schooltest_url)
}, error = function(e) {
  message("Error fetching page: ", schooltest_url)
  return(NULL)
})

if (!is.null(page)) {
  # Extract Religious Entity
  religious_entity <- page %>%
    html_nodes("p.max-w-2xl span.font-normal") %>% 
    html_text(trim = TRUE)
  
  # Extract Student Memorial Table
  table_node <- page %>% html_nodes("figure.wp-block-table table")

  if (length(table_node) > 0) {
    table_data <- table_node %>% 
      html_table(fill = TRUE) %>% 
      .[[1]]  # Extract first table if multiple exist
  } else {
    table_data <- tibble(Name = NA, Date_of_Death = NA)  # Placeholder if no table
  }

  # Print results
  print(glue::glue("Religious Entity: {religious_entity}"))
  print(table_data)
}

```


Now with all

```{r}
library(rvest)
library(tidyverse)
library(httr)

# Function to scrape a single school's details using its link

scrape_school_details <- function(school_link) {
  # Ensure the full URL is formed correctly
  full_url <- ifelse(grepl("^https?://", school_link), school_link, paste0("https://nctr.ca", school_link))
  
  # Read the page
  page <- tryCatch({
    read_html(full_url)
  }, error = function(e) {
    message("Error fetching page: ", full_url)
    return(NULL)
  })
  
  if (is.null(page)) return(tibble(school_link = school_link, religious_entity = NA, student_data = NA))
  
  # Extract Religious Entity
  religious_entity <- page %>%
    html_nodes("p.max-w-2xl span.font-normal") %>%
    html_text(trim = TRUE)
  
  # Extract Student Memorial Table
  table_node <- page %>% html_nodes("figure.wp-block-table table")
  
  if (length(table_node) > 0) {
    table_data <- table_node %>% 
      html_table(fill = TRUE) %>% 
      .[[1]]  # Extract first table if multiple exist
  } else {
    table_data <- tibble(Name = NA, Date_of_Death = NA)  # Placeholder if no table
  }
  
  tibble(
    school_link = school_link,
    religious_entity = ifelse(length(religious_entity) > 0, religious_entity, NA),
    student_data = list(table_data)  # Store tables in a list-column
  )
}

# Apply the scraping function to all school links from `all_schools`
detailed_school_data <- map_dfr(all_schools$school_link, scrape_school_details)

# View the results
print(detailed_school_data)

```

We join them by school_link

```{r}
# Ensure dplyr is loaded
library(dplyr)

# Join the two dataframes by the `school_link` column
combined_data <- left_join(all_schools, detailed_school_data, by = "school_link")

# View the combined data
print(combined_data)

```

We want to separate the details of each school into: school type, location and years_operation, and then remove the original column

```{r}
library(dplyr)
library(stringr)

combined_data <- combined_data %>%
  mutate(
    # Extract School Type (before " - ")
    school_type = str_extract(details, "^[^ -]+"),

    # Extract Location (between " - " and the year in parentheses)
    location = str_extract(details, "(?<= - ).*(?= \\()"),

    # Extract Years of Operation (in parentheses)
    years_operation = str_extract(details, "\\(.*\\)")
  ) %>%
  # Remove the original 'details' column
  select(-details)

# View the updated combined data with the new columns
print(combined_data)
```


We need to clean the years_operation column, there is duplicate data. Also, there are weird spaces within the numbers and we need to standardize them. We chose to remove the parentheses too:

```{r}
library(dplyr)
library(stringr)

combined_data <- combined_data %>%
  mutate(
    # Extract only the last set of parentheses with years
    years_operation = str_extract(years_operation, "\\([^()]*\\)$") %>%
      str_replace_all("[()]", "") %>%  # Remove parentheses
      str_trim() %>%  # Remove any leading or trailing spaces
      str_replace_all("\\s*[â€“â€”-]\\s*", "-")  # Standardize dashes to "-"
  )

# View the cleaned data
print(combined_data)
```


We're interested in seeing how many years each school was operating, so we extract the years from the column year_operation, we set them as numeric and we compute the substraction:

```{r}

library(dplyr)
library(stringr)

combined_data <- combined_data %>%
  mutate(
    # Extract the first and last year as numeric values
    start_year = as.numeric(str_extract(years_operation, "^\\d{4}")),
    end_year = as.numeric(str_extract(years_operation, "\\d{4}$")),
    
    # Calculate years in operation (if both values exist)
    years_active = ifelse(!is.na(start_year) & !is.na(end_year),
                          end_year - start_year,
                          NA)
  ) %>%


# View the cleaned data
print(combined_data)

```



## Second part: Interactive map

Next, we go to the Arcgis map, because Selenium can't scrape an iframe that leads to another website. We detect how many markers there are in layer 9, we remove any spinner hidden in the code, locate the popup when clicking on a marker and returning the info on Title, Longitude and Latitude.

Selenium navigates to the web, then clicks on the layers button, selects only the one we are interested in, then moves the map up 250 pixels and starts scraping.

```{r}
library(RSelenium)
library(stringr)

# Connect to Selenium running in Docker
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4449,
  browserName = "firefox"
)

remDr$open()

# Navigate to the ArcGIS map
remDr$navigate("https://www.arcgis.com/apps/webappviewer/index.html?id=0cc12b7f9f434dbaa7c2815aea84606e/")
Sys.sleep(5)  # Wait for page to load

# Click on Layers button
layers_button <- remDr$findElement(using = "css selector", "#dijit__WidgetBase_4")
layers_button$clickElement()
Sys.sleep(3)


# Click on Drop Menu
drop_menu <- remDr$findElement(using = "css selector", "#jimu_dijit_DropMenu_0")
drop_menu$clickElement()
Sys.sleep(3)


# Click on the second option in the drop menu
second_option <- remDr$findElement(using = "css selector", "#jimu_dijit_DropMenu_0 > div.drop-menu > div:nth-child(2)")
second_option$clickElement()
Sys.sleep(3)


# Click on the checkbox
checkbox <- remDr$findElement(using = "css selector", "#jimu_dijit_CheckBox_0 > div.checkbox.jimu-float-leading.jimu-icon.jimu-icon-checkbox")
checkbox$clickElement()
Sys.sleep(3)


# Click on the close button
close_button <- remDr$findElement(using = "css selector", "#_32_panel > div.jimu-panel-title.jimu-main-background.title-normal > div > div.close-btn.jimu-vcenter")
close_button$clickElement()
Sys.sleep(3)


# Move the map **UP by 250 pixels**
print("Moving map Up 250 pixels")
map_element <- remDr$findElement(using = "css selector", "#map")
remDr$mouseMoveToLocation(webElement = map_element)
remDr$buttondown()
remDr$mouseMoveToLocation(x = 0, y = 250)
remDr$buttonup()
Sys.sleep(2)  # Allow time for the map to move

# ðŸ“¸ Screenshot after moving the map
remDr$screenshot(file = "step_7_map_moved.png")

# **SCRAPING MARKERS WITH ZOOM-IN HANDLING**
school_data <- data.frame(Name = character(), Latitude = numeric(), Longitude = numeric(), stringsAsFactors = FALSE)

layer_9_markers <- remDr$findElements(using = "css selector", "#\\31 94bd5e080b-layer-9_layer > image")
print(paste("Layer 9 Markers found:", length(layer_9_markers)))

for (marker in layer_9_markers) {
  Sys.sleep(2)
  marker$clickElement()
  Sys.sleep(5)

  # ðŸ“¸ Screenshot after clicking marker
  remDr$screenshot(display = TRUE)  # Show image in RStudio Viewer

  popup <- tryCatch({
    remDr$findElement(using = "css selector", "div.esriPopupWrapper")
  }, error = function(e) {
    return(NULL)
  })

  # If popup is missing, zoom in, retry, and then zoom out
  if (is.null(popup)) {
    print("Popup not found, zooming in...")
    
    zoom_in_button <- remDr$findElement(using = "css selector", ".esriSimpleSliderIncrementButton")
    zoom_in_button$clickElement()
    Sys.sleep(3)

    marker$clickElement()
    Sys.sleep(3)

    # ðŸ“¸ Screenshot after zooming in
    remDr$screenshot(display = TRUE)

    popup <- tryCatch({
      remDr$findElement(using = "css selector", "div.esriPopupWrapper")
    }, error = function(e) {
      return(NULL)
    })

    # Zoom back out if we zoomed in
    if (!is.null(popup)) {
      zoom_out_button <- remDr$findElement(using = "css selector", ".esriSimpleSliderDecrementButton")
      zoom_out_button$clickElement()
      Sys.sleep(3)

      # ðŸ“¸ Screenshot after zooming out
      remDr$screenshot(display = TRUE)
    }
  }

  if (!is.null(popup)) {
    title_element <- popup$findChildElement(using = "css selector", ".header")
    title <- title_element$getElementText()[[1]]

    rows <- popup$findChildElements(using = "css selector", ".attrTable tr")

    latitude <- "N/A"
    longitude <- "N/A"

    for (row in rows) {
      cols <- row$findChildElements(using = "css selector", "td")

      if (length(cols) >= 2) {
        name_text <- cols[[1]]$getElementText()[[1]]
        value_text <- cols[[2]]$getElementText()[[1]]

        if (grepl("Latitude", name_text, ignore.case = TRUE)) {
          latitude <- value_text
        } else if (grepl("Longitude", name_text, ignore.case = TRUE)) {
          longitude <- value_text
        }
      }
    }

    print(paste("Title:", title, "Latitude:", latitude, "Longitude:", longitude))

    # Store data
    school_data <- rbind(school_data, data.frame(Name = title, Latitude = as.numeric(latitude), Longitude = as.numeric(longitude), stringsAsFactors = FALSE))

    # Close popup before moving to the next marker
    tryCatch({
      close_button <- remDr$findElement(using = "css selector", "div.titleButton.close")
      close_button$clickElement()
      Sys.sleep(2)
    }, error = function(e) {
      print("Popup close button not found, moving on...")
    })
  }
}

# Save data
write.csv(school_data, "school_data.csv", row.names = FALSE)

# Close session
remDr$close()


```
Count how many NAs we have

```{r}

# Count NAs in each column
na_count_per_column <- colSums(is.na(school_data))
print("NA count per column:")
print(na_count_per_column)

# Count total NAs in the entire dataframe
total_na_count <- sum(is.na(school_data))
print(paste("Total NAs in the dataframe:", total_na_count))

```


